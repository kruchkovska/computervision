{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cb_lab_3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXcCmyOeVYkQ",
        "outputId": "60e29a40-55ed-4401-a5c0-b9f109fdfb73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class_types = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "print(class_types[train_labels[0]])\n",
        "plt.imshow(train_images[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "pTMZaeQXVbPn",
        "outputId": "258110e9-c247-410c-812a-1de94995d16f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ankle boot\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f8e65ae4a90>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_shape =  np.concatenate((test_images.shape[1:], [1]))\n",
        "CLASSES = len(np.unique(test_labels))\n",
        "\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "metadata": {
        "id": "R-9G9vDpVdJC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def create_pairs(x, digit_indices):\n",
        "    '''\n",
        "    Positive and negative pair creation.\n",
        "    Alternates between positive and negative pairs.\n",
        "    '''\n",
        "    pairs = []\n",
        "    labels = []\n",
        "    n = min([len(digit_indices[d]) for d in range(CLASSES)]) - 1\n",
        "    for d in range(CLASSES):\n",
        "        for i in range(n):\n",
        "            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n",
        "            pairs += [[x[z1], x[z2]]]\n",
        "            inc = random.randrange(1, CLASSES)\n",
        "            dn = (d + inc) % CLASSES\n",
        "            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n",
        "            pairs += [[x[z1], x[z2]]]\n",
        "            labels += [1, 0]\n",
        "    return np.array(pairs), np.array(labels)\n",
        "\n",
        "def make_pair_dataset(images, labels):\n",
        "    digit_indices = [np.where(labels == i)[0] for i in range(CLASSES)]\n",
        "    pairs, y = create_pairs(images, digit_indices)\n",
        "    return pairs, y"
      ],
      "metadata": {
        "id": "ZsdM80b6Vn7y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_pairs, tr_y = make_pair_dataset(train_images, train_labels)\n",
        "te_pairs, te_y = make_pair_dataset(test_images, test_labels)"
      ],
      "metadata": {
        "id": "SJZ0yv--Vp0Z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tr_y = tr_y.astype(np.float32)\n",
        "te_y = te_y.astype(np.float32)\n",
        "\n",
        "x_tra, x_val, y_tra, y_val = train_test_split(tr_pairs, tr_y, test_size=0.2, stratify=tr_y, random_state=21)"
      ],
      "metadata": {
        "id": "o1lEG5xhVr7L"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def res_identity(x, filters): \n",
        "  #renet block where dimension doesnot change.\n",
        "  #The skip connection is just simple identity conncection\n",
        "  #we will have 3 blocks and then input will be added\n",
        "\n",
        "  x_skip = x # this will be used for addition with the residual block \n",
        "  f1, f2 = filters\n",
        "\n",
        "  #first block \n",
        "  x = tf.keras.layers.Conv2D(f1, kernel_size=(1, 1), strides=(1, 1), padding='valid')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "\n",
        "  #second block # bottleneck (but size kept same with padding)\n",
        "  x = tf.keras.layers.Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "\n",
        "  # third block activation used after adding the input\n",
        "  x = tf.keras.layers.Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  # x = Activation(activations.relu)(x)\n",
        "\n",
        "  # add the input \n",
        "  x = tf.keras.layers.Add()([x, x_skip])\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "\n",
        "  return x"
      ],
      "metadata": {
        "id": "cqlv79glVt69"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def res_conv(x, s, filters):\n",
        "  '''\n",
        "  here the input size changes''' \n",
        "  x_skip = x\n",
        "  f1, f2 = filters\n",
        "\n",
        "  # first block\n",
        "  x = tf.keras.layers.Conv2D(f1, kernel_size=(1, 1), strides=(s, s), padding='valid')(x)\n",
        "  # when s = 2 then it is like downsizing the feature map\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "\n",
        "  # second block\n",
        "  x = tf.keras.layers.Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "\n",
        "  #third block\n",
        "  x = tf.keras.layers.Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "  # shortcut \n",
        "  x_skip = tf.keras.layers.Conv2D(f2, kernel_size=(1, 1), strides=(s, s), padding='valid')(x_skip)\n",
        "  x_skip = tf.keras.layers.BatchNormalization()(x_skip)\n",
        "\n",
        "  # add \n",
        "  x = tf.keras.layers.Add()([x, x_skip])\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "\n",
        "  return x"
      ],
      "metadata": {
        "id": "PfPG-dAzVwLQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Resnet50():\n",
        "\n",
        "  input_im = tf.keras.layers.Input(shape=img_shape)\n",
        "  x = tf.keras.layers.ZeroPadding2D(padding=(3, 3))(input_im)\n",
        "\n",
        "  # 1st stage\n",
        "  # here we perform maxpooling, see the figure above\n",
        "\n",
        "  x = tf.keras.layers.Conv2D(64, kernel_size=(7, 7), strides=(2, 2))(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(tf.keras.activations.relu)(x)\n",
        "  x = tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
        "\n",
        "  #2nd stage \n",
        "  # frm here on only conv block and identity block, no pooling\n",
        "\n",
        "  x = res_conv(x, s=1, filters=(64, 256))\n",
        "  x = res_identity(x, filters=(64, 256))\n",
        "  x = res_identity(x, filters=(64, 256))\n",
        "\n",
        "  # 3rd stage\n",
        "\n",
        "  x = res_conv(x, s=2, filters=(128, 512))\n",
        "  x = res_identity(x, filters=(128, 512))\n",
        "  x = res_identity(x, filters=(128, 512))\n",
        "  x = res_identity(x, filters=(128, 512))\n",
        "\n",
        "  # 4th stage\n",
        "\n",
        "  x = res_conv(x, s=2, filters=(256, 1024))\n",
        "  x = res_identity(x, filters=(256, 1024))\n",
        "  x = res_identity(x, filters=(256, 1024))\n",
        "  x = res_identity(x, filters=(256, 1024))\n",
        "  x = res_identity(x, filters=(256, 1024))\n",
        "  x = res_identity(x, filters=(256, 1024))\n",
        "\n",
        "  # 5th stage\n",
        "\n",
        "  x = res_conv(x, s=2, filters=(512, 2048))\n",
        "  x = res_identity(x, filters=(512, 2048))\n",
        "  x = res_identity(x, filters=(512, 2048))\n",
        "\n",
        "  # ends with average pooling and dense connection\n",
        "\n",
        "  x = tf.keras.layers.AveragePooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "  x = tf.keras.layers.Flatten()(x)\n",
        "  x = tf.keras.layers.Dense(CLASSES, activation='softmax', kernel_initializer='he_normal')(x) #multi-class\n",
        "\n",
        "  # define the model \n",
        "\n",
        "  model = tf.keras.Model(inputs=input_im, outputs=x, name='Resnet50')\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "b5Kw3-FFVwzZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_model = Resnet50()"
      ],
      "metadata": {
        "id": "9NZ8arRhV0HN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_a_in = tf.keras.layers.Input(shape =img_shape, name = 'ImageA_Input')\n",
        "img_b_in = tf.keras.layers.Input(shape =img_shape, name = 'ImageB_Input')\n",
        "img_a_feat = feature_model(img_a_in)\n",
        "img_b_feat = feature_model(img_b_in)\n",
        "combined_features = tf.keras.layers.concatenate([img_a_feat, img_b_feat], name='merge_features')\n",
        "combined_features = tf.keras.layers.Dense(16, activation = 'linear')(combined_features)\n",
        "combined_features = tf.keras.layers.BatchNormalization()(combined_features)\n",
        "combined_features = tf.keras.layers.Activation('relu')(combined_features)\n",
        "combined_features = tf.keras.layers.Dense(4, activation = 'linear')(combined_features)\n",
        "combined_features = tf.keras.layers.BatchNormalization()(combined_features)\n",
        "combined_features = tf.keras.layers.Activation('relu')(combined_features)\n",
        "combined_features = tf.keras.layers.Dense(1, activation = 'sigmoid')(combined_features)"
      ],
      "metadata": {
        "id": "MbWDoE4TV2T8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_model = tf.keras.Model(inputs = [img_a_in, img_b_in], outputs=[combined_features], name = 'Similarity_Model')\n",
        "similarity_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79J4_srDWYK7",
        "outputId": "ae143bef-4617-45ad-d64f-4aeefa608dec"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Similarity_Model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " ImageA_Input (InputLayer)      [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " ImageB_Input (InputLayer)      [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " Resnet50 (Functional)          (None, 10)           23601930    ['ImageA_Input[0][0]',           \n",
            "                                                                  'ImageB_Input[0][0]']           \n",
            "                                                                                                  \n",
            " merge_features (Concatenate)   (None, 20)           0           ['Resnet50[0][0]',               \n",
            "                                                                  'Resnet50[1][0]']               \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 16)           336         ['merge_features[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_53 (BatchN  (None, 16)          64          ['dense_1[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_49 (Activation)     (None, 16)           0           ['batch_normalization_53[0][0]'] \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 4)            68          ['activation_49[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_54 (BatchN  (None, 4)           16          ['dense_2[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_50 (Activation)     (None, 4)            0           ['batch_normalization_54[0][0]'] \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 1)            5           ['activation_50[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 23,602,419\n",
            "Trainable params: 23,549,259\n",
            "Non-trainable params: 53,160\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['mae'])"
      ],
      "metadata": {
        "id": "JJHoCpyUWaBZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    min_delta=0,\n",
        "    patience=3,\n",
        "    verbose=0,\n",
        "    mode='auto',\n",
        "    baseline=None,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "save_model = tf.keras.callbacks.ModelCheckpoint(\n",
        "    \"/content/drive/MyDrive/Classroom/similarity_model\",\n",
        "    monitor='val_loss',\n",
        "    verbose=0,\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False,\n",
        ")\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=1, min_lr=0.0001)"
      ],
      "metadata": {
        "id": "e38DXXNEWbjR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCH = 17\n",
        "BS = 128\n",
        "history = similarity_model.fit([x_tra[:,0], x_tra[:, 1]], y_tra, epochs=EPOCH, batch_size=BS, \n",
        "                    validation_data=([x_val[:, 0], x_val[:, 1]], y_val), callbacks=[early_stopping, save_model, reduce_lr])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpNn2qRKWdnS",
        "outputId": "75493ef2-b611-4d7d-eb23-b5c434e853be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/17\n",
            "  3/750 [..............................] - ETA: 2:29:56 - loss: 0.7460 - mae: 0.4900"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_history(history):\n",
        "    plt.title('Train/validation loss')\n",
        "    plt.plot(history.history['loss'], label = 'training loss')\n",
        "    plt.plot(history.history['val_loss'], label = 'validation loss')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.ylim(0,1)\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "ZD5IWCffWfvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_model_output(nb_examples = 3):\n",
        "    pv_a, pv_b, pv_sim  = te_pairs[:nb_examples, 0], te_pairs[:nb_examples, 1], te_y[:nb_examples]\n",
        "\n",
        "    pred_sim = similarity_model.predict([pv_a, pv_b])\n",
        "    fig, m_axs = plt.subplots(2, pv_a.shape[0], figsize = (12, 6))\n",
        "    for c_a, c_b, c_d, p_d, (ax1, ax2) in zip(pv_a, pv_b, pv_sim, pred_sim, m_axs.T):\n",
        "\n",
        "        ax1.imshow(c_a)\n",
        "        ax1.set_title('Image A\\n Actual: %3.0f%%' % (100*c_d))\n",
        "        ax1.axis('off')\n",
        "        ax2.imshow(c_b)\n",
        "        ax2.set_title('Image B\\n Predicted: %3.0f%%' % (100*p_d))\n",
        "        ax2.axis('off')\n",
        "    return fig\n",
        "    # a completely untrained model\n",
        "_ = show_model_output(5)"
      ],
      "metadata": {
        "id": "-WZfFijiWho7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_similar(image, images, count=3, threshold=50):\n",
        "    pv_a, pv_b = np.reshape([image]*len(images), (len(images), 28,28)), np.reshape(images, (len(images), 28,28))\n",
        "    pred_sim = similarity_model.predict([pv_a, pv_b]).ravel()\n",
        "    indxs = np.argsort(pred_sim)[::-1][:count]\n",
        "    \n",
        "    fig, m_axs = plt.subplots(2, count, figsize = (12, 6))\n",
        "    i = 0\n",
        "    for c_a, (ax1, ax2) in zip(pv_a[:count], m_axs.T):\n",
        "        \n",
        "        ax1.imshow(c_a)\n",
        "        ax1.set_title('Input image\\n')\n",
        "        ax1.axis('off')\n",
        "        ax2.imshow(pv_b[indxs[i]])\n",
        "        ax2.set_title('Similar\\n Predicted: %3.0f%%' % (100*pred_sim[indxs[i]]))\n",
        "        ax2.axis('off')\n",
        "        i += 1"
      ],
      "metadata": {
        "id": "L7m6Y4OXWzAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_similar(test_images[1], images=test_images[1:50])"
      ],
      "metadata": {
        "id": "oRya4h6TW1aX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_similar(image, images, count=3, threshold=50):\n",
        "    pv_a, pv_b = np.reshape([image]*len(images), (len(images), 28,28)), np.reshape(images, (len(images), 28,28))\n",
        "    pred_sim = similarity_model.predict([pv_a, pv_b]).ravel()\n",
        "    indxs = np.argsort(pred_sim)[::-1][:count]\n",
        "    \n",
        "    fig, m_axs = plt.subplots(2, count, figsize = (12, 6))\n",
        "    i = 0\n",
        "    for c_a, (ax1, ax2) in zip(pv_a[:count], m_axs.T):\n",
        "        \n",
        "        ax1.imshow(c_a)\n",
        "        ax1.set_title('Input image\\n')\n",
        "        ax1.axis('off')\n",
        "        ax2.imshow(pv_b[indxs[i]])\n",
        "        ax2.set_title('Similar\\n Predicted: %3.0f%%' % (100*pred_sim[indxs[i]]))\n",
        "        ax2.axis('off')\n",
        "        i += 1"
      ],
      "metadata": {
        "id": "fyKNuqp1W3sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_similar(test_images[1], images=test_images[1:50])"
      ],
      "metadata": {
        "id": "44PXpf09W5gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_features = feature_model.predict(test_images, verbose = True,batch_size=BS)"
      ],
      "metadata": {
        "id": "4lVHxfdVW8xZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from sklearn.manifold import TSNE\n",
        "tsne_obj = TSNE(n_components=2,\n",
        "                init='pca',\n",
        "                random_state=101,\n",
        "                method='barnes_hut',\n",
        "                n_iter=500,\n",
        "                verbose=2)\n",
        "tsne_features = tsne_obj.fit_transform(x_test_features)"
      ],
      "metadata": {
        "id": "9dUViOI6W-Ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj_categories = ['T-shirt/top','Trouser','Pullover','Dress',\n",
        "'Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot'\n",
        "]\n",
        "colors = plt.cm.rainbow(np.linspace(0, 1, 10))\n",
        "plt.figure(figsize=(10, 10))"
      ],
      "metadata": {
        "id": "PWmZSYZBXATp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for c_group, (c_color, c_label) in enumerate(zip(colors, obj_categories)):\n",
        "    plt.scatter(tsne_features[np.where(test_labels == c_group), 0],\n",
        "    tsne_features[np.where(test_labels == c_group), 1],\n",
        "    marker='o',\n",
        "    color=c_color,\n",
        "    linewidth='1',\n",
        "    alpha=0.8,\n",
        "    label=c_label)\n",
        "plt.xlabel('Dimension 1')\n",
        "plt.ylabel('Dimension 2')\n",
        "plt.title('t-SNE on Testing Samples')\n",
        "plt.legend(loc='best')\n",
        "plt.savefig('clothes-dist.png')\n",
        "plt.show(block=False)"
      ],
      "metadata": {
        "id": "oMiltwtPXCFc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}